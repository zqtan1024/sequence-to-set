label = genia_train
model_type = ssn
# model_path = bert-base-cased
# tokenizer_path = bert-base-cased
model_path = dmis-lab/biobert-large-cased-v1.1
tokenizer_path = dmis-lab/biobert-large-cased-v1.1
cpu = false
debug = false
train_path = data/datasets/genia/genia_train_dev_context.json
valid_path = data/datasets/genia/genia_test_context.json
types_path = data/datasets/genia/genia_types.json
train_batch_size = 6
eval_batch_size = 6
epochs = 30
lr = 2e-5
lr_warmup = 0.2
weight_decay = 0.01
max_grad_norm = 1.0
prop_drop = 0.1
store_predictions = true
store_examples = true
sampling_processes = 0
log_path = data/genia/main/
save_path = data/genia/main/
seed = 46
final_eval = false
init_eval = false
freeze_transformer = false
confidence = 0.4
num_query = 60
decoder_layers = 6
lstm_layers = 3
lstm_drop = 0.1
pos_size = 25
char_lstm_layers = 1
char_lstm_drop = 0.1
char_size = 50
use_glove = true
use_pos = true
use_char_lstm = true
pool_type = max
reduce_dim = true
bert_before_lstm = true
wordvec_path = ../biovec/PubMed-shuffle-win-30.txt

# label = ace05_train
# model_type = ssn
# # model_path = bert-base-cased
# # tokenizer_path = bert-base-cased
# model_path = bert-large-cased
# tokenizer_path = bert-large-cased
# cpu = false
# debug = false
# train_path = data/datasets/ace05/ace05_train_context.json
# valid_path = data/datasets/ace05/ace05_dev_context.json
# types_path = data/datasets/ace05/ace05_types.json
# train_batch_size = 12
# eval_batch_size = 12
# epochs = 120
# lr = 2e-5
# lr_warmup = 0.2
# weight_decay = 0.01
# max_grad_norm = 1.0
# prop_drop = 0.1
# store_predictions = true
# store_examples = true
# sampling_processes = 0
# log_path = data/ace05/main/
# save_path = data/ace05/main/
# seed = 47
# final_eval = false
# init_eval = false
# freeze_transformer = false
# confidence = 0.6
# num_query = 60
# decoder_layers = 3
# lstm_layers = 3
# lstm_drop = 0.2
# pos_size = 25
# char_lstm_layers = 1
# char_lstm_drop = 0.1
# char_size = 50
# use_glove = true
# use_pos = true
# use_char_lstm = true
# pool_type = max
# reduce_dim = true
# bert_before_lstm = true
# wordvec_path = ../glove/glove.6B.100d.txt

# label = ace04_train
# model_type = ssn
# # model_path = bert-base-cased
# # tokenizer_path = bert-base-cased
# model_path = bert-large-cased
# tokenizer_path = bert-large-cased
# cpu = false
# debug = false
# train_path = data/datasets/ace04/ace04_train_context.json
# valid_path = data/datasets/ace04/ace04_dev_context.json
# types_path = data/datasets/ace04/ace04_types.json
# train_batch_size = 8
# eval_batch_size = 8
# epochs = 120
# lr = 2e-5
# lr_warmup = 0.2
# weight_decay = 0.01
# max_grad_norm = 1.0
# prop_drop = 0.1
# store_predictions = true
# store_examples = true
# sampling_processes = 0
# log_path = data/ace04/main/
# save_path = data/ace04/main/
# seed = 47
# final_eval = false
# init_eval = false
# freeze_transformer = false
# confidence = 0.5
# num_query = 60
# decoder_layers = 6
# lstm_layers = 3
# lstm_drop = 0.2
# pos_size = 25
# char_lstm_layers = 1
# char_lstm_drop = 0.1
# char_size = 50
# use_glove = true
# use_pos = true
# use_char_lstm = true
# pool_type = max
# reduce_dim = true
# bert_before_lstm = true
# wordvec_path = ../glove/glove.6B.100d.txt

# label = kbp17_train
# model_type = ssn
# # model_path = bert-base-cased
# # tokenizer_path = bert-base-cased
# model_path = bert-large-cased
# tokenizer_path = bert-large-cased
# cpu = false
# debug = false
# train_path = data/datasets/kbp17/kbp17_train_context.json
# valid_path = data/datasets/kbp17/kbp17_dev_context.json
# types_path = data/datasets/kbp17/kbp17_types.json
# train_batch_size = 6
# eval_batch_size = 6
# epochs = 120
# lr = 2e-5
# lr_warmup = 0.2
# weight_decay = 0.01
# max_grad_norm = 1.0
# prop_drop = 0.1
# store_predictions = true
# store_examples = true
# sampling_processes = 0
# log_path = data/kbp17/main/
# save_path = data/kbp17/main/
# seed = 47
# final_eval = false
# init_eval = false
# freeze_transformer = false
# confidence = 0.4
# num_query = 60
# decoder_layers = 3
# lstm_layers = 3
# lstm_drop = 0.2
# pos_size = 25
# char_lstm_layers = 1
# char_lstm_drop = 0.1
# char_size = 50
# use_glove = true
# use_pos = true
# use_char_lstm = true
# pool_type = max
# reduce_dim = true
# bert_before_lstm = true
# wordvec_path = ../glove/glove.6B.100d.txt